# pip install feature_engine
import pandas as pd  #Pandas library for data manipulation and analysis.
from sklearn.model_selection import train_test_split #used to split the dataset into training and testing subsets.
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet # used to build and evaluate the predictive models.
from sklearn.pipeline import Pipeline #used to create a pipeline of data preprocessing and modeling steps
from sklearn.preprocessing import MinMaxScaler #used to standardize the input features and encode categorical features, respectively
from sklearn.compose import ColumnTransformer #used to apply different preprocessing steps to different columns of the input data.
from sklearn.impute import SimpleImputer # used to fill missing values in the input data
from feature_engine.outliers import Winsorizer # used to apply different preprocessing steps to different columns of a Pandas DataFrame
from sklearn.preprocessing import OneHotEncoder # used to encode categorical features as one-hot vectors
from sklearn.metrics import r2_score #used to evaluate the performance of the predictive models
import joblib #used to save and load trained models as binary files.
import pickle #used to save and load trained models as binary files.
import numpy as np #for numerical computations
import matplotlib.pyplot as plt #for data visualization
import sweetviz
# import pyodbc #pyodbc is a Python library that provides an interface for connecting to and 
# #interacting with databases using ODBC (Open Database Connectivity)

# conn = pyodbc.connect('Driver={SQL Server Native Client 11.0};'
#                       'Server=LAPTOP-F819LJM9\SQLEXPRESS;'
#                       'Database=digitmg;'
#                       'Trusted_Connection=yes;')
# SQL Integration
from sqlalchemy import create_engine
from urllib.parse import quote 

user_name = 'root'
database = 'lsr'
your_password = 'Sai@123kumar'
engine = create_engine(f'mysql+pymysql://{user_name}:%s@localhost:3306/{database}' % quote(f'{your_password}'))
# Load the offline data into Database 
startup = pd.read_csv('50_Startups (1).csv')
# # adver.info()
startup.to_sql('startup', con = engine, if_exists='replace', chunksize=1000, index = False)
df = pd.read_sql_query('SELECT * FROM startup', engine)
df.head() # display first 5 rows
# Perform univariate analysis
print(df.describe())
# Perform bivariate analysis
corr = df.corr()
print(corr)
# AutoEDA
# Automated Libraries
# import sweetviz
my_report = sweetviz.analyze([df, "df"])
my_report.show_html('Report.html')
# check for missing values
df.isna().sum()
df.columns
X = df.iloc[:,0:4] 
X.head()
Y = df['Profit']
numeric_features = X.select_dtypes(exclude = ['object']).columns

numeric_features
num_pipeline = Pipeline([('impute', SimpleImputer(strategy = 'mean')), ('scale', MinMaxScaler()) ])
# Encoding on State column
categorical_features = ['State']

categorical_features
# Convert Categorical data "State" to Numerical data using OneHotEncoder

categ_pipeline = Pipeline([('encoding', OneHotEncoder(sparse_output=False))])
# Using ColumnTransfer to transform the columns of an array or pandas DataFrame. This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space.
preprocess_pipeline = ColumnTransformer([('numerical', num_pipeline, numeric_features),('categorical', categ_pipeline, categorical_features)])
imp_enc_scale = preprocess_pipeline.fit(X)  # Pass the raw data through pipeline

imp_enc_scale
joblib.dump(imp_enc_scale, 'imp_enc_scale')
import os 
os.getcwd()
df1 = pd.DataFrame(imp_enc_scale.transform(X), columns =  imp_enc_scale.get_feature_names_out()) # Clean and processed data
df1.describe()
df1.iloc[:,0:3].columns
# Multiple boxplots in a single visualization.
#Columns with larger scales affevt other columns.
#Below code ensures each column gets its own y-axis.
# pandas plot() function with parameters kind ='box' and subplots = True

df1.iloc[:,0:3].plot(kind ='box', subplots = True, sharey = False, figsize = (15, 8))

''' sharey True or 'all': x- or y-axis will be shared among all subplots.
False or 'none': each subplot x- or y-axis will be independent.'''

#increase spacing between subplots
plt.subplots_adjust(wspace = 0.75)# ws is the width of the padding between subplots,as the fraction of average axis width.
plt.show()
winsor = Winsorizer(capping_method = 'iqr', # choose IQR rule boundaries or
                    tail = 'both', # cap left,right or both tails
                    fold = 1.5,
                    variables = list(df1.iloc[:,0:3].columns))

# Fit the data
winz_data = winsor.fit(df1[list(df1.iloc[:,0:3].columns)])

#save the pipeline
joblib.dump(winz_data, 'winsor')

df1[list(df1.iloc[:,0:3].columns)] = winz_data.transform(df1[list(df1.iloc[:,0:3].columns)])
df1.iloc[:,0:3].plot(kind ='box', subplots = True, sharey = False, figsize = (15, 8))

''' sharey True or 'all': x- or y-axis will be shared among all subplots.
False or 'none': each subplot x- or y-axis will be independent.'''

#increase spacing between subplots
plt.subplots_adjust(wspace = 0.75)# ws is the width of the padding between subplots,as the fraction of average axis width.
plt.show()
Y.head()
# train_test_split is used to split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(df1, Y, test_size = 0.2, random_state = 0)
X_train.shape
X_test.shape
# Fit a linear regression model
regressor = LinearRegression()
regressor.fit(X_train, Y_train)
# Evaluate the model
# Evaluate the model with train data

pred_train = regressor.predict(X_train)  # Predict on train data

pred_train
# Predict on test set and evaluate performance
y_pred = regressor.predict(X_test)
r2_score_linear = r2_score(Y_test, y_pred)
# Fit a Lasso regression model with L1 regularization
lasso = Lasso(alpha=0.01)
lasso.fit(X_train, Y_train)
# Predict on test set and evaluate performance
y_pred_lasso = lasso.predict(X_test)
r2_score_lasso = r2_score(Y_test, y_pred_lasso)
# Fit a Ridge regression model with L2 regularization
ridge = Ridge(alpha=0.01)
ridge.fit(X_train, Y_train)

# Predict on test set and evaluate performance
y_pred_ridge = ridge.predict(X_test)
r2_score_ridge = r2_score(Y_test, y_pred_ridge)
 # Create an instance of the Elastic Net model
enet = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net_model = ElasticNet(alpha=0.1, l1_ratio=0.5)
# Fit the model on the training data
enet.fit(X_train, Y_train)
# Predict on the test data
y_pred = enet.predict(X_test)
# Evaluate the model performance
r2_score_elastic_net = r2_score(Y_test, y_pred )

# Print R-squared scores for each model
print('Linear Regression R-squared:', r2_score_linear)
print('Lasso Regression R-squared:', r2_score_lasso)
print('Ridge Regression R-squared:', r2_score_ridge)
print('ElasticNet R-squared:', r2_score_elastic_net)
# GridSearchCV is a method in scikit-learn that allows you to search over a grid of hyperparameters for the best combination of parameters for a given model
from sklearn.model_selection import GridSearchCV
# Lasso Regression
# from sklearn.model_selection import GridSearchCV

lasso = Lasso()

parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5 ,10, 20]}
lasso_reg = GridSearchCV(lasso, parameters, scoring = 'r2', cv = 5)
lasso_reg.fit(X_train, Y_train)
lasso_reg.best_params_
lasso_reg.best_score_
y_pred_lasso = lasso_reg.predict(X_test)
# Adjusted r-square#
Grid_lasso = lasso_reg.score(X_train,Y_train)
Grid_lasso
# Ridge Regression
# from sklearn.model_selection import GridSearchCV
# from sklearn.linear_model import Ridge

ridge = Ridge()

parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5 ,10, 20]}
ridge_reg = GridSearchCV(ridge, parameters, scoring = 'r2', cv = 5)
ridge_reg.fit(X_train, Y_train)

ridge_reg.best_params_
ridge_reg.best_score_
ridge_pred = ridge_reg.predict(X_test)
# Adjusted r-square#
Grid_ridge = ridge_reg.score(X_train, Y_train)
Grid_ridge
# ElasticNet Regression
# from sklearn.model_selection import GridSearchCV
# from sklearn.linear_model import ElasticNet
enet = ElasticNet()

# parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5 ,10, 20]}

enet_reg = GridSearchCV(enet, parameters, scoring = 'r2', cv = 5)

enet_reg.fit(X_train, Y_train)
enet_reg.best_params_
enet_reg.best_score_
enet_pred = enet_reg.predict(X_test)
# Adjusted r-square
Grid_elasticnet = enet_reg.score(X_train,Y_train)
Grid_elasticnet
scores_all = pd.DataFrame({'models':['Lasso', 'Ridge', 'Elasticnet', 'Grid_lasso', 'Grid_ridge', 'Grid_elasticnet'], 'Scores':[r2_score_lasso, r2_score_ridge, r2_score_elastic_net, Grid_lasso, Grid_ridge,Grid_elasticnet]})
scores_all
'Best score obtained is for Gridsearch Lasso Regression'
finalgrid = lasso_reg.best_estimator_
finalgrid
# Pickle is a Python module used to convert a Python object into a byte stream representation, which can be saved to a file or transferred over a network
# Save the best model
pickle.dump(finalgrid, open('grid_lasso.pkl', 'wb'))
