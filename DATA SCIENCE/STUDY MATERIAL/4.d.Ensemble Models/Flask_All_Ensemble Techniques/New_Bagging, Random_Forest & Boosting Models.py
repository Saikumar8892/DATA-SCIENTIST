'''
CRISP-ML(Q):

Business Understanding: 

Business Problem: Heavy competition in winning Oscar awards
Business Objective: Maximize the Oscar winning chances
Business Constraint: Minimize the Production Cost

Success Criteria:
Business: Increase Oscar winning chances by at least 30%
Machine Learning: Achieve an accuracy of atleast 70%
Economic: Cost benefit of around 300MUSD becasue of excess revenue collection from OTT

Data Understanding: 
Dimensions: 505 rows * 19 cols
Marketing expense	
Production expense	
Multiplex coverage	
Budget	
Movie_length	
Lead_ Actor_Rating	
Lead_Actress_rating	
Director_rating	
Producer_rating	
Critic_rating	
Trailer_views	
3D_available	
Time_taken	
Twitter_hastags	
Genre	
Avg_age_actors	
Num_multiplex	
Collection	
Oscar (Target)
'''

import pandas as pd  # For data manipulation and analysis
import numpy as np  # For numerical computations
import matplotlib.pyplot as plt  # For creating visualizations
import seaborn as sns  # For statistical data visualization

from sklearn.compose import ColumnTransformer  # For column-wise transformations
from sklearn.pipeline import Pipeline  # For creating a data processing pipeline
from sklearn.impute import SimpleImputer  # For imputing missing values
from sklearn.preprocessing import MinMaxScaler  # For scaling numerical features
from sklearn.preprocessing import OneHotEncoder  # For one-hot encoding categorical features
from feature_engine.outliers import Winsorizer  # For outlier handling using winsorization

from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets

import joblib  # For saving and loading objects using joblib
import pickle  # For serializing and deserializing Python objects

from sklearn import tree  # For decision tree classifier
from sklearn.ensemble import BaggingClassifier  # For bagging classifier
from sklearn.ensemble import RandomForestClassifier  # For random forest classifier
from sklearn.ensemble import AdaBoostClassifier  # For AdaBoost classifier
from sklearn.ensemble import GradientBoostingClassifier  # For gradient boosting classifier
# pip install xgboost
import xgboost as xgb  # For using XGBoost library for gradient boosting

from sklearn.metrics import confusion_matrix  # For computing confusion matrix
from sklearn.metrics import accuracy_score  # For computing accuracy score
from sklearn.model_selection import cross_validate  # For cross-validation

# Hyperparameter optimization
from sklearn.model_selection import RandomizedSearchCV  # For randomized search cross-validation
from sklearn.model_selection import GridSearchCV  # For grid search cross-validation

import sklearn.metrics as skmet  # For various metrics such as accuracy score, confusion matrix, etc.


data = pd.read_csv(r"movies_classification.csv")

from sqlalchemy import create_engine  # For creating database connections
from urllib.parse import quote
# Database credentials
user = 'user1'  # Database username
pw = quote('amer@mysql')  # Database password
db = 'titan'  # Database name

# Creating an engine to connect to the database
engine = create_engine(f"mysql+pymysql://{user}:{pw}@localhost/{db}")

# The table name should be in lower case
data.to_sql('movies_tbl', con=engine, if_exists='replace', chunksize=1000, index=False)

# SQL query to select all data from the 'movies_tbl' table
sql = "SELECT * FROM movies_tbl;"

# Read data from the SQL database into a DataFrame using the SQL query and engine
df = pd.read_sql_query(sql, engine)

# Display the first few rows of the DataFrame
df.head()

# Display information about the DataFrame, including the data types of columns and missing values
df.info()

# AutoEDA
# D-Tale
#########

# pip install dtale
import dtale  # For interactive data exploration and analysis

# Create a D-Tale instance and visualize the DataFrame
d = dtale.show(df)

# Open D-Tale in the default web browser
d.open_browser()

# AutoEDA with Sweetviz
import sweetviz  # For automatic exploratory data analysis

# Analyze the DataFrame using Sweetviz
my_report = sweetviz.analyze(df)

# Save the HTML report generated by Sweetviz
my_report.show_html('Report1.html')
#########


# Input and Output Split
predictors = df.loc[:, df.columns != "Oscar"]  # Extracting predictor variables by excluding the column "Oscar"
type(predictors)  # Checking the data type of the predictors DataFrame

target = df["Oscar"]  # Extracting the target variable ("Oscar" column)
type(target)  # Checking the data type of the target Series

target #Print Target Value

# Segregating Non-Numeric features
categorical_features = predictors.select_dtypes(include=['object']).columns  # Selecting columns with object dtype as categorical features
categorical_features  # Displaying the names of categorical features

# Segregating Numeric features
numeric_features = predictors.select_dtypes(exclude=['object']).columns  # Selecting columns with non-object dtype as numeric features
numeric_features  # Displaying the names of numeric features

# Checking for missing values in the DataFrame
df.isnull().sum()  # Returns the number of missing values in each column of the DataFrame

# Define pipeline for handling missing data using mean imputation and scaling
num_pipeline = Pipeline(steps=[('impute', SimpleImputer(strategy='mean')), ('scale', MinMaxScaler())])

# Check unique values and value counts of categorical features
print(predictors['3D_available'].unique().size)
print(predictors['3D_available'].value_counts())
print(predictors['Genre'].unique().size)
print(predictors['Genre'].value_counts())

# Define pipeline for encoding categorical features using OneHotEncoder
encoding_pipeline = Pipeline([('onehot', OneHotEncoder(drop='first', sparse_output = False))])

# Define column transformer to apply the encoding_pipeline to categorical features
preprocess_pipeline = ColumnTransformer([('numerical', num_pipeline, numeric_features),('categorical', encoding_pipeline, categorical_features)])

ise = preprocess_pipeline.fit(predictors)

# Save the scaling transformer for future use
joblib.dump(ise, 'ise') # imputation, scaling and encoding

# Transform the cleaned data to obtain scaled data
clean_data = pd.DataFrame(ise.transform(predictors), columns=ise.get_feature_names_out())
# Check for missing values in the clean data
clean_data.isnull().sum()

# ## Outlier Analysis
# get_ipython().run_line_magic('matplotlib', 'inline')
# import matplotlib.pyplot as plt
# import seaborn as sns
# Multiple boxplots in a single visualization.
# Columns with larger scales affect other columns. 
# Below code ensures each column gets its own y-axis.

# pandas plot() function with parameters kind = 'box' and subplots = True

ax = clean_data.iloc[:,:16].plot(kind = 'box', subplots = True, sharey = False, figsize = (25, 18)) 
'''sharey True or 'all': x- or y-axis will be shared among all subplots.
False or 'none': each subplot x- or y-axis will be independent.'''
# Set x-axis labels rotation and fontsize for each subplot
for subplot in ax:
    subplot.set_xticklabels(subplot.get_xticklabels(), rotation=90, fontsize=21)
# Adjust the spacing between subplots
plt.subplots_adjust(wspace=0.9)
# Show the plot
plt.show()

# Define Winsorizer with specified parameters
winsor = Winsorizer(capping_method='iqr',  # Use IQR rule boundaries
                    tail='both',  # Cap both tails
                    fold=1.5,  # Fold factor for determining the capping limits
                    variables=list(clean_data.iloc[:,:16].columns))  # Apply winsorization to all columns

# Fit Winsorizer to the clean data
outlier = winsor.fit(clean_data.iloc[:,:16])

# Save the winsorizer model
joblib.dump(outlier, 'winsor')

# Transform the clean data using the winsorizer
clean_data[list(clean_data.iloc[:,:16].columns)] = outlier.transform(clean_data[list(clean_data.iloc[:,:16].columns)])

# Plot box plots of the cleaned data
ax = clean_data.iloc[:,:16].plot(kind = 'box', subplots = True, sharey = False, figsize = (25, 18)) 
'''sharey True or 'all': x- or y-axis will be shared among all subplots.
False or 'none': each subplot x- or y-axis will be independent.'''
# Set x-axis labels rotation and fontsize for each subplot
for subplot in ax:
    subplot.set_xticklabels(subplot.get_xticklabels(), rotation=90, fontsize=21)
# Adjust the spacing between subplots
plt.subplots_adjust(wspace=0.9)
# Show the plot
plt.show()

# Splitting data into training and testing data set
X_train, X_test, Y_train, Y_test = train_test_split(clean_data, target, test_size = 0.2, 
                                                    stratify = target, random_state = 0) 

# # Bagging Classifier Model
# from sklearn.ensemble import BaggingClassifier
# decision tree defined first
# from sklearn import tree
clftree = tree.DecisionTreeClassifier()
# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import accuracy_score


# Define Bagging Classifier with specified parameters
bag_clf = BaggingClassifier(estimator=clftree,  # Base estimator (Decision Tree)
                            n_estimators=500,  # Number of base estimators
                            bootstrap=True,  # Whether to bootstrap samples
                            n_jobs=-1,  # Number of jobs to run in parallel
                            random_state=42)  # Random state for reproducibility

# Fit the Bagging Classifier on the training data
bagging = bag_clf.fit(X_train, Y_train)

# Evaluate performance on the training set
print(confusion_matrix(Y_train, bagging.predict(X_train)))
print(accuracy_score(Y_train, bagging.predict(X_train)))
print('\n')

# Evaluate performance on the testing set
print(confusion_matrix(Y_test, bagging.predict(X_test)))
print(accuracy_score(Y_test, bagging.predict(X_test)))

# Saving the trained model
pickle.dump(bagging, open('baggingmodel.pkl', 'wb'))

# Define function to perform cross-validation and return scores

def cross_validation(model, _X, _y, _cv=5):
    """
    Function to perform cross-validation and return the results as a DataFrame.
    
    Args:
    - model: Estimator object to be cross-validated.
    - _X: Feature matrix.
    - _y: Target vector.
    - _cv: Number of folds for cross-validation (default is 5).
    
    Returns:
    - DataFrame containing the cross-validation results.
    """
    _scoring = ['accuracy', 'precision', 'recall', 'f1']
    # Perform cross-validation
    results = cross_validate(estimator=model,
                             X=_X,
                             y=_y,
                             cv=_cv,
                             scoring=_scoring,
                             return_train_score=True)

    # Create a DataFrame to store the results
    return pd.DataFrame({
        "Training Accuracy scores": results['train_accuracy'],
        "Mean Training Accuracy": results['train_accuracy'].mean() * 100,
        "Training Precision scores": results['train_precision'],
        "Mean Training Precision": results['train_precision'].mean(),
        "Training Recall scores": results['train_recall'],
        "Mean Training Recall": results['train_recall'].mean(),
        "Training F1 scores": results['train_f1'],
        "Mean Training F1 Score": results['train_f1'].mean(),
        "Validation Accuracy scores": results['test_accuracy'],
        "Mean Validation Accuracy": results['test_accuracy'].mean() * 100,
        "Validation Precision scores": results['test_precision'],
        "Mean Validation Precision": results['test_precision'].mean(),
        "Validation Recall scores": results['test_recall'],
        "Mean Validation Recall": results['test_recall'].mean(),
        "Validation F1 scores": results['test_f1'],
        "Mean Validation F1 Score": results['test_f1'].mean()
    })


# Perform cross-validation for Bagging Classifier
Bagging_cv_scores = cross_validation(bag_clf, X_train, Y_train, 5)

# Define function to plot cross-validation results
def plot_result(x_label, y_label, plot_title, train_data, val_data):
    """
    Function to plot the cross-validation results.
    
    Args:
    - x_label: Label for the x-axis.
    - y_label: Label for the y-axis.
    - plot_title: Title of the plot.
    - train_data: Training data scores.
    - val_data: Validation data scores.
    """
    # Set the size of the plot
    plt.figure(figsize=(12, 6))
    
    # Define the labels for the x-axis (folds)
    labels = ["1st Fold", "2nd Fold", "3rd Fold", "4th Fold", "5th Fold"]
    
    # Define the x-axis range
    X_axis = np.arange(len(labels))
    
    # Set the y-axis limits
    plt.ylim(0.40000, 1)
    
    # Plot the training data
    plt.bar(X_axis - 0.2, train_data, 0.1, color='blue', label='Training')
    
    # Plot the validation data
    plt.bar(X_axis + 0.2, val_data, 0.1, color='red', label='Validation')
    
    # Set the plot title
    plt.title(plot_title, fontsize=30)
    
    # Set the x-axis ticks and labels
    plt.xticks(X_axis, labels)
    
    # Set the x-axis label
    plt.xlabel(x_label, fontsize=14)
    
    # Set the y-axis label
    plt.ylabel(y_label, fontsize=14)
    
    # Add a legend
    plt.legend()
    
    # Add grid lines
    plt.grid(True)
    
    # Show the plot
    plt.show()

# Plot cross-validation results for Bagging Classifier
plot_result("Fold", "Score", "Bagging Classifier Cross-Validation", 
            Bagging_cv_scores["Mean Training Accuracy"], Bagging_cv_scores["Mean Validation Accuracy"])

# Set up inline plotting
# get_ipython().run_line_magic('matplotlib', 'inline')

# Define model name for plotting
model_name = "Bagging Classifier"

# Plot accuracy scores in 5 Folds for Bagging Classifier
plot_result(model_name,
            "Accuracy",
            "Accuracy scores in 5 Folds",
            Bagging_cv_scores["Training Accuracy scores"],
            Bagging_cv_scores["Validation Accuracy scores"])

# Import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

# Initialize RandomForestClassifier
rf_Model = RandomForestClassifier()

# Define hyperparameter values to tune
n_estimators = [int(x) for x in np.linspace(start=10, stop=80, num=10)]  # Number of trees in the forest
max_features = ['auto', 'sqrt']  # Number of features to consider when looking for the best split
max_depth = [2, 4]  # Maximum depth of the tree
min_samples_split = [2, 5]  # Minimum number of samples required to split an internal node
min_samples_leaf = [1, 2]  # Minimum number of samples required to be at a leaf node
bootstrap = [True, False]  # Method of selecting samples for training each tree


# Define a list of number of trees to try
n_estimators = [int(x) for x in np.linspace(start=10, stop=80, num=10)]

# Print the list of number of trees
print(n_estimators)

# Create the parameter grid
param_grid = {'n_estimators': n_estimators,
              'max_features': max_features,
              'max_depth': max_depth,
              'min_samples_split': min_samples_split,
              'min_samples_leaf': min_samples_leaf,
              'bootstrap': bootstrap}

# Print the parameter grid
print(param_grid)

# Hyperparameter optimization with GridSearchCV
rf_Grid = GridSearchCV(estimator=rf_Model, param_grid=param_grid, cv=10, verbose=1, n_jobs=-1)
rf_Grid.fit(X_train, Y_train)

# Get the best parameters
rf_Grid.best_params_

# Get the best estimator
cv_rf_grid = rf_Grid.best_estimator_

# Check Accuracy
# Evaluation on Test Data
test_pred = cv_rf_grid.predict(X_test)

# Calculate accuracy on test data
accuracy_test = np.mean(test_pred == Y_test)
accuracy_test

# Compute confusion matrix
cm = skmet.confusion_matrix(Y_test, test_pred)

# Plot confusion matrix
cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Oscar Nominated', 'Not Nominated'])
cmplot.plot()
cmplot.ax_.set(title='Oscar Nomination Detection Confusion Matrix', xlabel='Predicted Value', ylabel='Actual Value')

# Print train and test accuracy
print(f'Train Accuracy: {rf_Grid.score(X_train, Y_train):.3f}')
print(f'Test Accuracy: {rf_Grid.score(X_test, Y_test):.3f}')


# ### Hyperparameter optimization with RandomizedSearchCV
# Perform hyperparameter optimization with RandomizedSearchCV
rf_Random = RandomizedSearchCV(estimator=rf_Model, param_distributions=param_grid, cv=10, verbose=2, n_jobs=-1)
rf_Random.fit(X_train, Y_train)

# Get the best parameters
best_params_random = rf_Random.best_params_
print("Best Parameters (Randomized Search):", best_params_random)

# Get the best estimator
best_estimator_random = rf_Random.best_estimator_

# Evaluation on Test Data
test_pred_random = best_estimator_random.predict(X_test)

# Calculate accuracy on test data
accuracy_test_random = np.mean(test_pred_random == Y_test)
print("Test Accuracy (Randomized Search):", accuracy_test_random)

# Compute confusion matrix
cm_random = skmet.confusion_matrix(Y_test, test_pred_random)

# Plot confusion matrix
cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm_random, display_labels=['Oscar Nominated', 'Not Nominated'])
cmplot.plot()
cmplot.ax_.set(title='Oscar Nomination Detection Confusion Matrix', xlabel='Predicted Value', ylabel='Actual Value')

# Print train and test accuracy
train_accuracy_random = rf_Random.score(X_train, Y_train)
test_accuracy_random = rf_Random.score(X_test, Y_test)
print(f'Train Accuracy (Randomized Search): {train_accuracy_random:.3f}')
print(f'Test Accuracy (Randomized Search): {test_accuracy_random:.3f}')


# ## Save the best model from Randomsearch CV approach
pickle.dump(best_estimator_random, open('rfc.pkl', 'wb'))


# ## Cross Validation implementation
# from sklearn.model_selection import cross_validate


# Perform cross-validation for Random Forest model
Random_forest_result = cross_validation(best_estimator_random, X_train, Y_train, 5)
Random_forest_result

# Define the model name for plotting
model_name = "RandomForestClassifier"

# Plot the accuracy scores for training and validation data
plot_result(model_name,
            "Accuracy",
            "Accuracy scores in 5 Folds",
            Random_forest_result["Training Accuracy scores"],  # Training accuracy scores
            Random_forest_result["Validation Accuracy scores"])  # Validation accuracy scores

# #  AdaBoosting
# from sklearn.ensemble import AdaBoostClassifier

# Initialize AdaBoostClassifier with specified parameters
ada_clf = AdaBoostClassifier(learning_rate=0.02, n_estimators=5000)

# Fit AdaBoostClassifier on training data
ada_clf1 = ada_clf.fit(X_train, Y_train)

# Make predictions on testing data
predictions = ada_clf1.predict(X_test)

# Evaluate model performance on testing data
confusion_matrix(Y_test, predictions)  # Compute confusion matrix
accuracy_score(Y_test, predictions)  # Compute accuracy score

# Evaluate model performance on training data
accuracy_score(Y_train, ada_clf1.predict(X_train))  # Compute accuracy score

# Save the trained AdaBoost model
pickle.dump(ada_clf1, open('adaboost.pkl', 'wb'))

# Initialize GradientBoostingClassifier
boost_clf = GradientBoostingClassifier()

# Fit GradientBoostingClassifier on training data
boost_clf1 = boost_clf.fit(X_train, Y_train)

# Make predictions on testing data
grad_pred = boost_clf1.predict(X_test)

# Evaluate model performance on testing data
print(confusion_matrix(Y_test, grad_pred))  # Compute confusion matrix
print(accuracy_score(Y_test, grad_pred))  # Compute accuracy score

# Evaluate model performance on training data
print(confusion_matrix(Y_train, boost_clf1.predict(X_train)))  # Compute confusion matrix
print(accuracy_score(Y_train, boost_clf1.predict(X_train)))  # Compute accuracy score


# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier

# Initialize GradientBoostingClassifier with specified hyperparameters
boost_clf2 = GradientBoostingClassifier(learning_rate=0.02, n_estimators=1000, max_depth=1)

# Fit GradientBoostingClassifier with specified hyperparameters on training data
boost_clf_p = boost_clf2.fit(X_train, Y_train)

# Make predictions on testing data using the trained model with specified hyperparameters
grad_pred_p = boost_clf_p.predict(X_test)

# Evaluate model performance on testing data with specified hyperparameters
print(confusion_matrix(Y_test, grad_pred_p))  # Compute confusion matrix
print('\n')
print(accuracy_score(Y_test, grad_pred_p))  # Compute accuracy score

# Evaluate model performance on training data with specified hyperparameters
print(confusion_matrix(Y_train, boost_clf_p.predict(X_train)))  # Compute confusion matrix
accuracy_score(Y_train, boost_clf_p.predict(X_train))  # Compute accuracy score

# Save the trained GradientBoosting model with specified hyperparameters to a file
pickle.dump(boost_clf_p, open('gradiantboostparam.pkl', 'wb'))

# Load the saved GradientBoosting model with specified hyperparameters from the file
grad_model_p = pickle.load(open('gradiantboostparam.pkl', 'rb'))

## XGBoosting
# pip install xgboost
# import xgboost as xgb

# Initialize XGBClassifier with specified hyperparameters
xgb_clf = xgb.XGBClassifier(max_depth=5, n_estimators=10000, learning_rate=0.3, n_jobs=-1)

# Fit XGBClassifier with specified hyperparameters on the training data
xgb_clf1 = xgb_clf.fit(X_train, Y_train)

# Make predictions on the testing data using the trained XGBClassifier
xgb_pred = xgb_clf1.predict(X_test)

# Evaluate model performance on the testing data
print(confusion_matrix(Y_test, xgb_pred))  # Compute confusion matrix
accuracy_score(Y_test, xgb_pred)  # Compute accuracy score

# Plot feature importance
xgb.plot_importance(xgb_clf)
# Get feature importance values and create a DataFrame
fi = pd.DataFrame(xgb_clf1.feature_importances_.reshape(1, -1), columns=X_train.columns)
fi

# Save the trained XGBClassifier model to a file
pickle.dump(xgb_clf1, open('xgb.pkl', 'wb'))

# Load the saved XGBClassifier model from the file
xgb_model = pickle.load(open('xgb.pkl', 'rb'))


# Perform RandomizedSearchCV for XGBClassifier hyperparameters
xgb_clf = xgb.XGBClassifier(n_estimators=500, learning_rate=0.1, random_state=42)

# Define parameter grid for hyperparameter tuning
param_test1 = {'max_depth': range(3, 10, 2), 'gamma': [0.1, 0.2, 0.3],
               'subsample': [0.8, 0.9], 'colsample_bytree': [0.8, 0.9]}

# Initialize RandomizedSearchCV with defined parameter grid
xgb_RandomGrid = RandomizedSearchCV(estimator=xgb_clf,
                                     param_distributions=param_test1,
                                     cv=5, verbose=2, n_jobs=-1)

# Perform RandomizedSearchCV on training data
Randomized_search1 = xgb_RandomGrid.fit(X_train, Y_train)

# Get the best estimator from RandomizedSearchCV
cv_xg_clf = Randomized_search1.best_estimator_

# Make predictions on the testing data using the best estimator
randomized_pred = Randomized_search1.predict(X_test)

# Evaluate model performance on the testing data with the best estimator
accuracy_score(Y_test, randomized_pred)

# Get the best hyperparameters found by RandomizedSearchCV
Randomized_search1.best_params_

# Make predictions on the training data using the best estimator
randomized_pred_1 = Randomized_search1.predict(X_train)

# Evaluate model performance on the training data with the best estimator
accuracy_score(Y_train, randomized_pred_1)

# Save the best estimator (model with hyperparameters) to a file
pickle.dump(cv_xg_clf, open('Randomizedsearch_xgb.pkl', 'wb'))
